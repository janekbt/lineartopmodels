---
title: "Report"
author: "Nicolas Carmona, Niklas Tillenburg, Janek Teders"
date: "January 18, 2019"
output: pdf_document
subtitle: A predictive model of house prices in King County, USA
---

```{r setup, include=FALSE}
set.seed(1234)
#install.packages("kableExtra")
#install.packages("jpeg")
#install.packages("png")
#install.packages("kable")
#install.packages("gridExtra")
library(lubridate)
library(gridExtra)
library(MASS)
library(png)
library(jpeg)
library(tidyverse)
library(knitr)
library(car)
library(faraway)
library(broom)
knitr::opts_chunk$set(echo = FALSE, cache=TRUE, warning=FALSE)
library(kableExtra)
options("kableExtra.html.bsTable" = T)
options(scipen=999)

raw_data <- readRDS("raw_data.rds")
full_data <- readRDS("full_data.rds")
full_data_woo <- readRDS("full_data_woo.rds")

B <- 100
k <- 10
```


## The dataset

The data consists of house sale prices for King County area, Washington State, which includes Seattle. It contains houses sold between May 2014 and May 2015.

The dataset was obtained from https://www.kaggle.com/harlfoxem/housesalesprediction on December 2018.

- Houses sold between May 2014 and May 2015
- Observations: **`r nrow(raw_data)`**
- Variables: **`r ncol(raw_data)`**

The following map displays the geographical distribution of the data points. We can observe that the data is spread around most of the urban areas.

```{r, fig.align = "center", out.width = "300px", out.height = "300px"}
knitr::include_graphics("King_county_map.PNG")
```

## Variables

Below is a description of each one of the variables avaliable in the data:

* **id** - Unique ID for each home sold
* **date** - Date of the home sale 
* **price** - Price of each home sold 
* **bedrooms** - Number of bedrooms 
* **bathrooms** - Number of bathrooms, where .5 accounts for a room with a toilet but no shower 
* **sqft_living** - Square footage of the apartments interior living space 
* **sqft_lot** - Square footage of the land space 
* **floors** - Number of floors 
* **waterfront** - A dummy variable for whether the apartment was overlooking the waterfront or not 
* **view** - An index from 0 to 4 of how good the view of the property was 
* **condition** - An index from 1 to 5 on the condition of the apartment, 
* **grade** - An index from 1 to 13, where 1-3 falls short of building construction and design, 7 has an average level of construction and design, and 11-13 have a high quality level of construction and design. 
* **sqft_above** - The square footage of the interior housing space that is above ground level 
* **sqft_basement** - The square footage of the interior housing space that is below ground level 
* **yr_built** - The year the house was initially built 
* **yr_renovated** - The year of the houseâ€™s last renovation 
* **zipcode** - What zipcode area the house is in 
* **lat** - Latitude 
* **long** - Longitude 
* **sqft_living15** - The square footage of interior housing living space for the nearest 15 neighbors 
* **sqft_lot15** - The square footage of the land lots of the nearest 15 neighbors 


## Response variable: numeric and visual inspection

```{r include=FALSE}
Y_col = raw_data$price
```

Here we observe some descriptive statistics about the sale prices.

* **Min:** `r min(Y_col)`
* **1st quartile:** `r quantile(Y_col, 0.25)`
* **Median:** `r median(Y_col)`
* **3rd quartile:** `r quantile(Y_col, 0.75)`
* **Max:** `r max(Y_col)`
* **Mean (average):** `r round(mean(Y_col), 2)`
* **Standard deviatiion:** `r round(sd(Y_col), 2)`

By looking at the Median and Mean, we suspect right skeweness in the data. We verify this in the following visual inspection of the data:

```{r echo=FALSE, fig.align = "center", fig.width=10.3, fig.height=5.5}
par(mfrow = c(1,2))
truehist(raw_data$price,
         xlab = "price")
a <- qqPlot(raw_data$price,
            xlab = "quantiles",
            ylab = "")
```

Adittionally to confirming our suspicion of right skeweness, we observe in the qq-plot (on the right) that the distribution does not fit the theoretical quantiles of the normal distribution. This goes against the "utopian" assumption of normality for the response variable when attepmting linear regression analysis. 

In order to improve this, we apply a simple transformation to the response variable. The data is transformed to it's coresponding logarithm base 10. The following are the histogram and qq-plot of the transformed variable:

```{r echo=FALSE, fig.align = "center", fig.width=10.3, fig.height=5.5}
par(mfrow = c(1,2))
truehist(log10(raw_data$price),
         xlab = "price")
a <- qqPlot(log10(raw_data$price),
            xlab = "quantiles",
            ylab = "")
```


The transformation proves to be a huge improvement in terms of the normality assumption.

## Regarding model selection

In order to pick the "best" model, a criteria must be specified. The P-value presents the problem of multiple testing, and, since we will be conducting several statistical tests, the P-value will not be part of out decision criteria.

Given this, the following metrics will be used for model comparison instead:
 
* $AIC$
* $BIC$
* $R^2_{adj.}$
* $RMSE$
* $P-value$


## Preliminary data formating

It was necesary to do some initial formating in order to make sense of the data for a linear model, and to add value for the model. The modifications are the following:

  - square feet into square meters
  - waterfront and renovated into a factor variable
  - split date into week and month of the year
  - removed original date variable
  - removed id because it is not predictive
  - split dataset into two parts, 80% and 20% (more on this later)
  
```{r}
complete_data <- read_csv("kc_house_data.csv") %>%
  sample_n(nrow(.))

raw_data <- complete_data %>%
  slice((n()/5):n())

data_20p <- complete_data %>%
  slice(1:(n()/5))

full_data <- raw_data %>%
  mutate_at(
    vars(starts_with("sqft")),
    function(x) x * 0.092903
  ) %>%
  rename_at(
    vars(starts_with("sqft")),
    function(x) str_replace(x, "sqft", "sqm")
  ) %>%
  mutate(
    week_of_year = week(date),
    month_of_year = month(date),
    renovated = factor(ifelse(yr_renovated > 0, "yes", "no")),
    wasViewed = factor(ifelse(view > 0, 1, 0)),
    waterfront = factor(waterfront)
  ) %>%
  select(-id, -date, -starts_with("sqm"), starts_with("sqm"))


first_fifth_avg <- data_20p %>%
  group_by(zipcode) %>%
  summarise(mean_price_zip = mean(price))

full_data <- full_data %>%
  left_join(., first_fifth_avg, key = zipcode) %>%
  mutate(
    price = log10(price),
    mean_price_zip = log10(mean_price_zip)
  )
```

  
## The first model

In the first model only the useful original variables will be included with minimal formating to provide a baseline for comparison.

```{r}
data_wo_new_vars <- full_data %>%
  select(-wasViewed, -renovated, -mean_price_zip)

model_1 <- lm(price ~ ., data = data_wo_new_vars)
summary(model_1)
```

There are two peculiarities we can observe. In the first place we observe that the variable sqm_basement was not taken into account and consists solely of NAs. The second observation provides the reason for this behavior: A singularity was discovered. That means that sqm_basement is a linear combination of two or more other variables, sqm_living and sqm_above in this case. The decision to remove those variables will be dealt with during the experimentation phase.

## Experimentation

Given that we now have both a baseline model and defined model selection criteria, using a trial and error approach, we can start testing different approaches seeking to improve the model. Among those approaches we will attempt outlier detection, correlation analysis and different kinds of variable transformations. Let us start with outlier detection.

### Outliers

Outliers, depending on their impact, may have a detrimental effect on the capacity of a model to effectively fit itself to the general patterns of the data and not just the present sample, which decreases its predictive ability. To find those divergent data points we will have a look at their leverage and the cook's distance.

**Leverage**: The power to shift the model towards that specific data point
**Cook's distance**: Measures the aggregated influence of that observation on the fitted values

```{r}
par(mfrow = c(1,2))
plot(model_1, which=5)
plot(model_1, which=4)
```

Inspecting the cooks distance plot we find that there are 4 highly influential data points. For the following experiment we will attempt to fit a linear model without these aforementioned observations and compare it to our baseline model.

```{r}
cross_val <- function(data, B = 100, k = 10){
  n <- nrow(data)
  folds <- sample(rep(c(1:k), length.out = nrow(data)))
  results <- matrix(0, nrow = k, ncol = B)
  for (b in 1:B){
    for (i in 1:k) {
      training <- data[folds != i,]
      test <- data[folds == i,]

      lmo <- lm(data = training, price ~ .)
      predict <- predict(lmo, test)
    
      results[i,b] <- sum((10^test$price - 10^predict)^2)/length(test$price)
    }
  }
  m_rmse <- mean(sqrt(results))
  sd_rmse <- sd(sqrt(results))
  l <- list(distr = sqrt(results), m_rmse = m_rmse, sd_rmse = sd_rmse)
  return(l)
}

intermediate_1 <- full_data_woo %>%
  select(-wasViewed, -renovated, -mean_price_zip)

intermediate_2 <- data_wo_new_vars

model_2 <- intermediate_1 %>%
  lm(data = . , price ~ .)

cv_2 <- cross_val(intermediate_1)
cv_1 <- cross_val(intermediate_2)

p_value_1 <- tidy(t.test(cv_1$distr, cv_2$distr, var.equal = F)) %>% 
  select(p.value) %>% 
  unlist() %>% 
  formatC(format = "e") %>% 
  rbind(" ",.)

Mean_RMSE <- rbind(cv_1$m_rmse, cv_2$m_rmse)
SD_RMSE <- rbind(cv_1$sd_rmse, cv_2$sd_rmse)

cbind(AIC(model_1, model_2), Mean_RMSE, SD_RMSE, p_value_1) %>%
  kable() %>%
  kable_styling()
```


